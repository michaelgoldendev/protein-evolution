\newif\ifmbeformat
\mbeformatfalse

\ifmbeformat
	\documentclass[nogrid]{MBE}%
\else
	\documentclass[twocolumn]{article}
	\usepackage{geometry}
	\newgeometry{
		top=1in,
		bottom=1in,
		outer=1in,
		inner=1in,
	}
	\usepackage{natbib}
	\renewcommand{\baselinestretch}{1.25}		
	\usepackage{authblk}
\fi

\usepackage[normalem]{ulem}
\usepackage{xcolor} 
\newcommand{\revch}[1]{{\color{blue} #1}}
\newcommand{\revcom}[1]{{\color{orange} #1}}
\newcommand{\revrem}[1]{{\color{red} \sout{#1}}}

\usepackage{url}
\usepackage{cuted}
\usepackage{amsmath}
\usepackage{subfig}
%\usepackage{caption}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{caption}
\usepackage{verbatim}
\usepackage{lipsum}
\usepackage{enumerate}
\usepackage{enumitem}
%load any additional packages
\usepackage{amssymb}
\usepackage{scalerel,stackengine}
\stackMath
\newcommand\reallywidehat[1]{%
	\savestack{\tmpbox}{\stretchto{%
			\scaleto{%
				\scalerel*[\widthof{\ensuremath{#1}}]{\kern-.6pt\bigwedge\kern-.6pt}%
				{\rule[-\textheight/2]{1ex}{\textheight}}%WIDTH-LIMITED BIG WEDGE
			}{\textheight}% 
		}{0.5ex}}%
	\stackon[1pt]{#1}{\tmpbox}%
}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{bibentry}
%\usepackage{epstopdf} 
%\usepackage{xr}
%\usepackage[landscape,a4paper]{geometry}
\usepackage{booktabs} 
\usepackage{colortbl} 
%\usepackage{xcolor} 
%\usepackage{xfrac}
%\newcommand{ra}[1]{renewcommand{arraystretch}{#1}}
%\usepackage{pdflscape}
%\usepackage{longtable}
%\usepackage{stackrel}
\usepackage{hyperref}
%\usepackage{graphicx}
%\usepackage{svg}
\usepackage{setspace}
%\usepackage{refcheck}
%\usepackage{listings}
%\usepackage{color} 
\usepackage{array}
\usepackage{xr}
\externaldocument[supplementary-]{supplementary}


\newif\iffigures
\figurestrue

\ifmbeformat
\jshort{mst}

\volname{}

\jvolume{0}

\jvol{}

\jissue{0}

\pubyear{2013}

\mstype{Article}

\artid{012}

\access{Advance Access publication March 3, 2013}
\fi

\newcommand{\abstractext}{We present a probabilistic model of protein evolution that captures several important features of protein sequence and local structure. The key feature being  dependencies between neighbouring amino acid positions that are temporal in nature due to sequence mutations that occur during evolution. The model is trained on a large number of protein alignments and corresponding phylogenetic trees that represent the evolutionary history of the aligned proteins. This yields a model that acts as a rich prior distribution over protein evolution. Our model provides a complete probabilistic description of protein backbone structures using an angle and bond length representation. Structure evolution is modelled jointly with sequence, permitting ancestral structures and sequences to be reconstructed in a phylogenetically rigorous manner. Likewise, the model can perform homology modelling to predict the unknown local backbone structure of a known protein sequence using additional information from potentially large numbers of homologous proteins.  The model is highly flexible with respect to input, implying that arbitrary combinations of protein sequences and structures can be used when performing various inference tasks. Our current model does not capture global features of protein structure that are necessary for accurate homology modelling or reconstruction of ancestral three-dimensional structures. However, it is ultimately expected to be combined with protein structure prediction models that account for long-range structural interactions, but that do not account for evolutionary information.}
\begin{comment}
\newcommand{\abstractext}{We present a probabilistic model of protein evolution that captures several important features of protein sequence and local structure. The key feature being  dependencies between neighbouring amino acid positions that are temporal in nature due to sequence mutations that occur during evolution. The model is trained on a large number of protein alignments and corresponding phylogenetic trees that represent the evolutionary history of the aligned proteins. This yields a model that acts as a rich prior distribution over protein evolution that can be used to perform several important inference tasks. One such task being Bayesian reconstruction of ancestral virus protein sequences, which we demonstrate to have better accuracy than competing methods. The model provides a complete probabilistic description of each protein's backbone structure using an angle and bond length representation. Structure evolution is modelled jointly with sequence, permitting ancestral structures and sequences to be reconstructed in a phylogenetically rigorous manner. Likewise, the model can perform homology modelling to predict the unknown local backbone structure of a known protein sequence using additional information from potentially large numbers of homologous proteins.  The model is highly flexible with respect to input, implying that arbitrary combinations of protein sequences and structures can be used when performing various inference tasks. The current model does not capture global features of protein structure that are necessary for accurate homology modelling or reconstruction of ancestral three-dimensional structures. However, it is ultimately expected to be combined with protein structure prediction models that account for such long-range dependencies, but that do not account for evolutionary information that can substantially enhance predictions of structures.}
\end{comment}

\begin{document}

\title{Probabilistic modelling of protein structure evolution}

\ifmbeformat
\author[Golden et al.]{Michael \surname{Golden},$^{\ast,1}$, Jotun Hein,$^{2}$ Thomas Hamerlyck,$^{3}$ and Oliver Pybus,$^{1}$}

\address{
$^{1}$Department of Zoology, University of Oxford, UK\\
$^{2}$Department of Statistics, University of Oxford, UK\\
$^{3}$Bioinformatics Centre, Section for Computational and RNA Biology, Department of Biology and Image Section, Department of Computer Science, University of Copenhagen\\
}

\history{Received 13 July 2017; reviews returned 26 November 2017; accepted 30 November 2017}

\coresp{E-mail: golden@phylo.dev}

%\datade{Protein families were obtained from the HOMSTRAD database.}

\editor{Name Surname}

\else
\author[1]{Michael Golden}
\author[2]{Thomas Hamelryck}
\author[1]{Oliver Pybus}
\affil[1]{Department of Zoology, University of Oxford, UK}
%\affil[2]{Department of Statistics, University of Oxford, UK}
\affil[2]{Bioinformatics Centre, Section for Computational and RNA Biology, Department of Biology and Image Section, Department of Computer Science, University of Copenhagen}
\fi


\ifmbeformat
\abstract{\abstractext}
\keyword{Evolution, protein structure, probabilistic model}
\maketitle

\else

\onecolumn
\maketitle
\begin{abstract}
\normalsize
\abstractext
\end{abstract}
\twocolumn
\fi



\section{Introduction}
\begin{figure*}
	\centering
	\includegraphics[width=1.75\columnwidth]{figures/divergence.pdf}
	\caption{\revcom{I need to improve this figure}}%
	\label{fig:divergence}%
\end{figure*}

Biological sequences contain an abundance of information pertaining to the function and evolution of biological molecules. However, evolutionary models are needed to transform this information into useful biological knowledge. Models of sequence evolution have been successfully applied to predicting RNA structures \citep{sukosd2012ppfold}, inferring selection \citep{murrell2013fubar}, reconstructing ancestral proteins \citep{wilson2015using}, predicting contact networks \citep{giardina2017inference}, estimating dates and locations of spillover events \citep{dudas2018mers}, detecting genotype-phenotype associations \citep{escalera2018parallel}, amongst numerous other important applications. \revcom{MG: are there better references / better examples I could use in this paragraph}.

Evolutionary models are important because they allow one to account for correlations due to shared ancestry. If these correlations are not considered and the biological entities being compared are instead treated as independent observations, this can lead to false conclusions. One common problem is the `founder effect' \citep{bhattacharya2007founder}, where the presence of a feature in multiple extant sequences may appear significant, but is not when taking into consideration that the feature was likely inherited from the common ancestor of the sequences being analysed. On the other hand, these correlations can also be useful. For example, they can be used to predict the unknown structure of a sequence from a known structure, this is referred as homology modelling \citep{arnold2006swiss}.

These correlations are depicted in Figure~\ref{fig:divergence}. Notice how the time seperating two proteins determines the degree of sequence similarity and  structural similarity i.e. the degree of correlation. Also notice how proteins with low sequence identity (middle and right proteins) are easily discernible as structurally homologous. Although the two types of information can not be measured on the same scale, it is often said that `structure evolves more slowly than sequence'. This fact is used in homology modelling, and more recently in models of protein structure evolution to more accurately estimate divergence times and tree topologies of proteins with  highly diverged sequences \citep{challis2012stochastic,herman2014simultaneous}.

Despite the structural conformations of proteins being the primary determinant of their functions, only a few evolutionary models of protein structure have been developed compared to evolutionary models of sequence. This is likely due to the challenges in specifying a tractable model of how a complex three-dimensional continuous object evolves, compared to sequence which is discrete and one-dimensional. Furthermore, it is desirable to consider how structure evolves as a function of sequence, since protein sequences are the primary determinant of protein structure folding.

A key step in developing a model of protein structure evolution is selecting a suitable structure representation and a stochastic process that describes how protein structure evolves as function of sequence divergence. A standard way of representing structure is using the three-dimensional Cartesian coordinates of each atom in space, as is found in the PDB (Protein Data Bank, \citet{berman2000protein}) file format. Early works by \citet{gutin1994evolution} and \citet{grishin1997estimation} used a three-dimensional Cartesian coordinate representation of protein backbone atoms and diffusion processes to estimate how structural distance increases with sequence divergence. 

Probabilistic models by \citet{challis2012stochastic} and \citet{herman2014simultaneous} use the three-dimensional Cartesian coordinates of amino acid  $\text{C}_{\alpha}$ atoms and Ornstein--Uhlenbeck (OU) processes to explicitly model the evolution of $\text{C}_{\alpha}$ atoms.  A benefit of these probabilistic models is that they are able to rigorously account for sources of uncertainty, such as an unknown phylogeny or alignment. This in turn allows the phylogeny and alignment to be inferred using structure and sequence information. The authors show that using both types of information reduces the uncertainty associated with estimates of these quantities, enabling more diverged sequences to be analysed. These models have been recently extended to account for correlations between neighbouring amino acid positions \citep{larson2018modeling}.

\iffalse
For the sake of computational tractability, the aforementioned approaches treat the atomic coordinates as evolving independently of another. \citep{larson2018modeling}
\fi

A non-probabilistic approach by \citet{echave2008evolutionary} and \citet{echave2010perturbative} referred to as the Linearly Forced Elastic Network Model (LFENM)  treats protein structures as a collection of $\text{C}_{\alpha}$ atoms connected by spring forces. The major benefit of LFENMs is that they do not assume independence of atomic coordinates and take into account non-local structural dependencies. In their current formulation LFENMs do not capture the dependence of structure on sequence.

Our recent model, ETDBN \citep{Golden2017} uses a dihedral angle representation motivated by the non-evolutionary TorusDBN model \citep{boomsma2008generative, boomsma2014equilibrium}. ETDBN and TorusDBN represents a protein structure as a sequence of $(\phi,\psi)$ dihedral angle pairs \citep{frellsen2012towards}. A novel stochastic diffusion process  \citet{garciap2016diffusions} was developed for ETDBN to model the evolution of dihedral angles.

\iffalse
Additionally, a coupling was introduced such that an amino acid change can lead to a \textit{jump} in dihedral angles and a change in diffusion process. This allows the model to capture changes in amino acid that are directionally coupled with changes in dihedral angle or secondary structure. As in \citet{challis2012stochastic} and \citet{herman2014simultaneous}, the insertion and deletion (indel) evolutionary process is also modelled to account for alignment uncertainty \citep{thorne1992inching}.
\fi

The dihedral angle representation is informed by the chemical nature of peptide bonds. Each amino acid in a protein peptide chain is covalently bonded to the next via a peptide bond. Peptide bonds have a partial double bond nature that results in a planar configuration of atoms in space. This configuration allows the protein backbone structure to be largely described as a series of $\phi$ and $\psi$ dihedral angles that defines the relationship between the planes in three-dimensional space. Because each amino acid (except for the N and C terminus amino acids) is associated with a ($\phi$,$\psi$) dihedral angle pair, only a sequence alignment is necessary to compare structures. On the other hand, models that use Cartesian coordinates typically need to superimpose structures in addition to requiring a sequence alignment \citep{herman2014simultaneous}. Such superimposition introduces an additional source of uncertainty. A further advantage of the dihedral angle representation is that there are fewer degrees of freedom per amino acid and therefore typically fewer parameters required to model their evolution.

Most models of structural evolution ignore dependencies amongst sites because of the increased computational demand and model complexity associated with such models. These dependencies are expected to influence patterns of evolution, specifically patterns of amino acid substitution. The current model deals with local dependencies only -- dependencies that are expected to arise due to interactions between neighbouring amino acids, for example, between amino acids in an $\alpha$-helix. It does not account for global dependencies -- dependencies that result in the globular nature of proteins \citep{boomsma2008generative}. In our previous model, ETDBN, we attempted to model local dependencies only by using a Hidden Markov Model (HMM) to capture dependencies amongst neighbouring aligned positions. 

HMMs such as PASSML \citep{li1998passml} have been successfully used to predict protein secondary structure from aligned sequences; however, these models typically have the disadvantage that they assume a canonical secondary structure shared amongst all the sequences being analysed. This restricts analysis to closely related sequences where conservation of secondary structure is a reasonable assumption. Our previous model, ETDBN, does not assume a canonical secondary structure, but instead uses a phylogenetic HMM approach (similar to \citet{siepel2004combining}) that assumes dependencies between evolutionary processes at neighbouring aligned positions, which allows somewhat more diverged sequences to be analysed. The present model completely relaxes this assumption, by using a context-dependent model of evolution similar to \citet{robinson2003protein} and \citep{yu2006dependence}. This has the implication that  diverged sequences may be analysed, where the neighbouring dependencies at a particular site in one part of the tree may be very different from neighbouring dependencies at the same site in another part of the tree.

In this article we focus on applying probablistic modelling of structure evolution to the problem of homology modelling. One problem with an angle and bond length reprsentation, is that small errors in angle predictions can lead to large errors in 3D structure prediction. For example, a single incorrectly predicted angle can cause an entire structure to self-intersect in 3D. For this reason we do not attempt to reconstruct the complete 3D structure of a protein, but instead focus on accurately modelling the series of $\phi$ and $\psi$ dihedral angles, using an approach that accurately quantifies the models uncertainty in it's angle predictions. State-of-the-art machine learning models of protein structure prediction (DeepMind's AlphaFold and AlQuraishi's end-to-end differentiable model) utilise angle models for predicting an intermediate angle representation. These intermediate layers suffer the same problems with angle errors. However, these errors are corrected in subsequent layers by accounting for the global properties of protein structure. We therefore consider our approach an important stepping stone towards accurate 3D structure prediction. We intend to replace the intermediate angle representation used in state-of-the-art models with our model which additionally considers structural homology information. This is expected to lead substantial improvements in 3D structure prediction. Furthermore, the use of evolutionary model should additional permit accurate 3D reconstruction of ancestral protein structures.

\iffalse

\fi


\iffalse
Recently, several studies \citep{challis2012stochastic, herman2014simultaneous} have proposed joint stochastic models of evolution which take into account simultaneous alignment of protein sequence and structure. These studies point out the limitations of earlier non-probabilistic methods, which often rely on heuristic procedures to infer parameters of interest. A major disadvantage of using heuristic procedures is that they typically fail to account for sources of uncertainty. For example, relying on a single fixed alignment, which is highly unlikely to be the \textit{true} underlying alignment, may bias the inference of the posterior distribution over evolutionary trees.

We present a generative evolutionary model, ETDBN (Evolutionary Torus Dynamic Bayesian Network), for pairs of homologous proteins. ETDBN captures dependencies between sequence and structure evolution, accounts for alignment uncertainty, and models the local dependencies between aligned sites.

A key step in modelling protein structure evolution is selecting a suitable structural representation and corresponding evolutionary model. Early works by \citet{gutin1994evolution} and \citet{grishin1997estimation} represented protein structure using three-dimensional Cartesian coordinates of protein backbone atoms and used diffusions processes to model the relationship between structural distance (measured using RMSD) and sequence similarity. More recent publications by \citet{challis2012stochastic} and \citet{herman2014simultaneous} likewise used the three-dimensional Cartesian coordinates of amino acid  $\text{C}_{\alpha}$ atoms to represent protein structure and additionally used Ornstein--Uhlenbeck (OU) processes to construct Bayesian probabilistic models of protein structure evolution. These models emphasise estimation of evolutionary parameters such as the evolutionary time between species, tree topologies and alignment, and attempt to fully account for sources of uncertainty. For the sake of computational tractability, the aforementioned approaches treat the Cartesian coordinates associated with atoms as evolving independently of another. A non-probabilistic approach by \citet{echave2008evolutionary} and \citet{echave2010perturbative} referred to as the Linearly Forced Elastic Network Model (LFENM)  treats protein structures as a collection of $\text{C}_{\alpha}$ atoms connected by spring forces. The major benefit of LFENMs is that they do not assume independence of atomic coordinates and take into account non-local structural dependencies. In their current formulation LFENMs do not capture the dependencies between amino acid sequence and structure and therefore do not account for the variable effect of sequence mutation on protein structure evolution.

Rather than using a Cartesian coordinate representation, our model, ETDBN, uses a dihedral angle representation motivated by the non-evolutionary TorusDBN model \citep{boomsma2008generative, boomsma2014equilibrium}. TorusDBN represents a single protein structure as a sequence of $(\phi,\psi)$ dihedral angle pairs, which are modelled using continuous bivariate angular distributions \citep{frellsen2012towards}. Likewise, ETDBN treats protein structure  as a random walk in space, again making use of the $\phi$ and $\psi$ dihedral angles (Figure~\ref{fig:model-structure} above).

The dihedral angle representation is informed by the chemical nature of peptide bonds. Each amino acid in a protein peptide chain is covalently bonded to the next via a peptide bond. Peptide bonds have a partial double bond nature that results in a planar configuration of atoms in space. This configuration allows the protein backbone structure to be largely described as a series of $\phi$ and $\psi$ dihedral angles that defines the relationship between the planes in three-dimensional space. Because each amino acid (except for the N and C terminus amino acids) is associated with a ($\phi$,$\psi$) dihedral angle pair, only a sequence alignment is necessary to compare structures. On the other hand, models that use Cartesian coordinates typically need to superimpose structures in addition to requiring a sequence alignment \citep{herman2014simultaneous}. Such superimposition introduces an additional source of uncertainty. A further advantage of the dihedral angle representation is that there are fewer degrees of freedom per amino acid and therefore typically fewer parameters required to model their evolution.


A novel stochastic diffusion process developed in \citet{garciap2016diffusions} was used to model the evolution of dihedral angles. Additionally, a coupling was introduced such that an amino acid change can lead to a \textit{jump} in dihedral angles and a change in diffusion process. This allows the model to capture changes in amino acid that are directionally coupled with changes in dihedral angle or secondary structure. As in \citet{challis2012stochastic} and \citet{herman2014simultaneous}, the insertion and deletion (indel) evolutionary process is also modelled to account for alignment uncertainty \citep{thorne1992inching}.

The Ornstein--Uhlenbeck processes used in \citet{challis2012stochastic} and \citet{herman2014simultaneous} ignore bond lengths and treat $\text{C}_{\alpha}$ atoms as evolving independently for the sake of computationally tractability. Furthermore, the OU process makes Gaussian assumptions. From a generative perspective these properties will lead to evolved proteins with $\text{C}_{\alpha}$ atoms that are unnaturally dispersed in space. Bond lengths are also ignored in ETDBN, but can be plausibly fixed, or modelled. As a result, it is expected that the use of angular diffusions will more naturally capture the underlying local protein structure manifold; notwithstanding, global structural constraints are not imposed which may also lead to simulated proteins that lack certain global properties of protein structure, such as protein compactness.

Tree-like dependencies typically arise when considering two or more homologous proteins related via a common ancestor. These dependencies manifest themselves most noticeably in the degree of amino acid sequence similarity between the homologous proteins. The strength of these dependencies is assumed to be a result of two major factors: the time since the common ancestor and the rate of evolution.

Failing to account for evolutionary dependencies can lead to false conclusions \citep{felsenstein1985phylogenies}, whereas accounting for evolutionary dependencies allows information from homologous proteins to be incorporated in a principled manner. This can lead to more accurate inferences, such as the prediction of a protein structure from a homologous protein sequence and structure, known as homology modelling \citep{arnold2006swiss}. Stochastic models such as ETDBN are not expected to compete with homology modelling software such as SWISS-MODEL \citep{arnold2006swiss}. However, they allow for estimation of evolutionary parameters and statements about uncertainty to be made in a statistically rigorous manner.

Most models of structural evolution ignore dependencies amongst sites because of the increased computational demand and model complexity associated with such models. These dependencies are expected to influence patterns of evolution, specifically patterns of amino acid substitution. The current model deals with local dependencies only -- dependencies that are expected to arise due to interactions between neighbouring amino acids, for example, between amino acids in an $\alpha$-helix. ETDBN does not account for global dependencies -- dependencies that result in the globular nature of proteins \citep{boomsma2008generative}. In ETDBN, we attempt to model local dependencies only by using a Hidden Markov Model (HMM) to capture dependencies amongst neighbouring aligned positions. HMMs such as PASSML \citep{li1998passml} have been successfully used to predict protein secondary structure from aligned sequences; however, these models typically have the disadvantage that they assume a canonical secondary structure shared amongst all the sequences being analysed. This restricts analysis to closely related sequences where conservation of secondary structure is a reasonable assumption. ETDBN does not assume a canonical secondary structure, but instead uses a phylogenetic HMM approach (similar to \citet{siepel2004combining}) that assumes dependencies between evolutionary processes at neighbouring aligned positions.

Parameters of ETDBN were estimated using 1200 homologous protein pairs from the HOMSTRAD database \citep{mizuguchi1998homstrad}. The resulting model provides a realistic prior distribution over proteins and protein structure evolution in comparison to previous stochastic models. Doing so enables biological insights into the relationship between sequence and structure evolution, such as patterns of amino acid change that are informative of patterns of structural change \citep{grishin2001fold}. It was with these features in mind that ETDBN was developed.
\fi

%%% Methods %%%
\section{Methods}

\subsection{Model}
\begin{figure*}
	\centering
	\includegraphics[width=1.75\columnwidth]{figures/model-structure-alt-final.pdf}
	\caption{Above: a depiction of a protein backbone (three amino acids long) with the $\omega$, $\phi$ and $\psi$ dihedral angles and the three additional bond angles ($\tau^{(1)}_{i}$, $\tau^{(2)}_{i}$, $\tau^{(3)}_{i}$) shown. Bond lengths are implicit. Bond angles and bond lengths are not to scale. Also shown are $\text{C}_{\alpha}$ atoms which attach to the amino acid side-chains. Each amino acid side-chain determines the characteristic nature of each amino acid. Every amino acid position corresponds to a hidden node in the model below.\newline
	Below: Graphical depiction of the model architecture showing three amino acid positions ($i-1$, $i$, and $i+1$) at two time instants ($t_0$ and $t_1$) along a single branch of a phylogenetic tree. Note that $S_{i}(t)=\big(H_{i}(t),A_{i}(t)\big)$.}%
	\label{fig:model-structure}%
\end{figure*}

\subsubsection{Model of a single protein}
A single protein consisting of $n$ amino acids:
\begin{align*}
 P_a  & = (H_a, A_a, X_a)\\
 & =\langle (H^{1}_a, A^{1}_a,X^{1}_a),\ldots,(H^{n}_a, A^{n}_a,X^{n}_a) \rangle
\end{align*}
is a sequence of aligned sites where each site $i$ is associated with a discrete-valued hidden state, $H_a^{i}$ (taking on one of $h$ possible values), a discrete-valued amino acid observation, $A_a^{i}$ (representing one of the twenty possible amino acids), and a corresponding vector of continuous-valued structural observations $X_a^{i}$ representing the backbone structure of the protein.

\paragraph{Structural observations}
The set of structural observations, $X_a^{i}$, at a particular site, $i$, consists of nine continuous-valued variables: three dihedral angles ($\phi_{i},\psi_{i},\omega_{i}$), three additional bond angles ($\tau^{(1)}_{i}=\reallywidehat{C{\alpha}_{i-1},C_{i-1},N_{i}}$, $\tau^{(2)}_{i}=\reallywidehat{C_{i-1},N_{i},C{\alpha}_{i}}$, $\tau^{(3)}_{i}=\reallywidehat{N_{i},C{\alpha}_{i},C_{i}}$), and three bond lengths ($b_{i}^{(1)}=\overrightarrow{C_{i-1},N_{i}}$, $b_{i}^{(2)}=\overrightarrow{N_{i},C{\alpha}_{i}}$, $b_{i}^{(3)}=\overrightarrow{C{\alpha}_{i},C_{i}}$). 

Note that $\phi_{1}$, $\tau^{(1)}_{1}$, and $\tau^{(2)}_{1}$, are undefined at the first position in the peptide backbone of an unaligned protein. Similarly, $\psi_{n}$ and  $\omega_{n}$ are undefined for the last position, $n$, in each unaligned protein.

Given the structural observations, $X_{a}$, it is possible to exactly reconstruct the three-dimensional coordinates of each atom in a protein's backbone \citep{parsons2005practical}.

The $\phi_{i}$ and $\psi_{i}$ dihedral angles are assumed to be drawn from a bivariate von Mises (bvM) distribution with mean vector $\mu_{\phi,\psi}=\langle \mu_{\phi},\mu_{\psi} \rangle$ and covariance parameters $\kappa_{\phi,\psi}=\langle \kappa_{1},\kappa_{2},\kappa_{3} \rangle$:
\begin{equation}
\label{eq:phipsi_dist}
\begin{array}{ccc}
(\phi_{i},\psi_{i}) & \sim & \text{bvM}\big(\mu_{\phi,\psi}(H_{a}^{i},A_{a}^{i}),\,\ensuremath{\kappa_{\phi,\psi}(H_{a}^{i},A_{a}^{i})}\big),
\end{array}
\end{equation}
where $\kappa_{1}$ is the variance associated with $\phi$, $\kappa_{2}$ is the variance associated with $\psi$, and $\kappa_{3}$ is the correlation between $\phi$ and $\psi$,

The $\omega_{i}$ dihedral angle (which determines the cis/trans conformation) at each site $i$ is assumed to be distributed according a univariate von Mises (vM) distribution with mean $\mu_{\omega}$ and concentration parameter $\kappa_{\omega}$ conditional on the hidden state $H_{a}^{i}$ and the amino acid $A_{a}^{i}$:
\begin{equation}
\label{eq:omega_dist}
\begin{array}{ccc}
\omega_{i} & \sim & \text{vM}\big(\mu_{\omega}(H_{a}^{i},A_{a}^{i}),\,\ensuremath{\kappa_{\omega}(H_{a}^{i},A_{a}^{i})}\big).
\end{array}
\end{equation}


The three additional bond angles ($\tau^{(1)}_{i}$, $\tau^{(2)}_{i}$, $\tau^{(3)}_{i}$) are each distributed according a univariate (vM) distribution conditional on the hidden state $H_{a}^{i}$ only:
\begin{equation}
\label{eq:angle_density}
\begin{array}{ccc}
\tau^{(1)}_{i} & \sim & \text{vM}\big(\mu_{\tau^{(1)}}(H_{a}^{i}),\,\ensuremath{\kappa_{\tau^{(1)}}(H_{a}^{i})}\big)
\\
\tau^{(2)}_{i} & \sim & \text{vM}\big(\mu_{\tau^{(2)}}(H_{a}^{i}),\,\ensuremath{\kappa_{\tau^{(2)}}(H_{a}^{i})}\big)
\\
\tau^{(3)}_{i} & \sim & \text{vM}\big(\mu_{\tau^{(3)}}(H_{a}^{i}),\,\ensuremath{\kappa_{\tau^{(3)}}(H_{a}^{i})}\big).
\end{array}
\end{equation}

The three bond lengths ($b_{i}^{(1)}>0$, $b_{i}^{(2)}>0$, $b_{i}^{(3)}>0$) are 
distributed according a truncated Multivariate Normal (MVN)
with mean vector, $\mathbf{\mu}$, of length 3 and a $3 \times 3$ covariance matrix, $\Sigma$, conditional on the hidden state $H_{a}^{i}$:
\begin{equation}
\label{eq:bond_density}
(b_{i}^{(1)}, b_{i}^{(2)}, b_{i}^{(3)})  \sim \text{MVN}\big(\mu(H_a^{i}),\,\Sigma(H_a^{i})\big).
\end{equation}
Note that the parameters in \eqref{eq:angle_density} and \eqref{eq:bond_density} are no longer conditional upon the amino acid observations $A_{a}^{i}$. This was done to reduce the number of model parameters, as the values of the three bond angles and three bond lengths are all largely invariant, implying that the values can be reasonably fixed. Despite this, we still opted to treat them as random variables so that the model gives a complete probabilistic description of a protein backbone structure.

\paragraph{Site likelihood}
The likelihood of a structural observation, $p\big(X_{a}^{i}\,|\,H_{a}^{i},A_{a}^{i},\hat{\theta}\big)$, at site $i$ conditional on the hidden state, $H^{i}_a$, and amino acid, $S^{i}_a$, is given by a product of the densities in Equations~\ref{eq:omega_dist}, \ref{eq:phipsi_dist}, \ref{eq:angle_density}, and \ref{eq:bond_density}.

\begin{figure*}
	\centering
	\includegraphics[width=2.0\columnwidth]{figures/ramachandran_empirical_and_model.pdf}
	\caption{Ramachandran plots comparing distributions of $\phi$ and $\psi$ angle combinations under three different amino acid contexts  (glycine, alanine, and alanine before proline). The top row shows the empirical distribution of $\phi$ and $\psi$ angles in our training datasets. The bottom row shows the distribution given by our model. Glycine (column 1) has the smallest amino acid side-chain and is therefore the least constrained. Alanine (column 2) has a larger side-chain than glycine, constraining most $\phi$ and $\psi$ angle to lie near a single peak. When alanine precedes proline (column 3) in the peptide backbone, the $\phi$ and $\psi$ angle combinations previously favoured become sterically hindered \revcom{MG: is `sterically hinderred' the correct terminology?}, favouring angle combinations at multiple peaks.}%
	\label{fig:ramachandranempirical}%
\end{figure*}

\paragraph{Hidden Markov model}
A sequence of structural observations representing a single protein backbone structure is modelled using a Hidden Markov Model (HMM). Hidden states in the HMM are primarily intended encode the angle and bond lengths distributions and their association with the different amino acids as specified in Equations~\ref{eq:omega_dist}-\ref{eq:bond_density}. However, the HMM is also critically to capture neighbouring dependencies, such as steric effects on dihedral angle conformations (Figure~\ref{fig:ramachandranempirical}). These neighbouring dependencies are captured using a $h\times{h}$ transition probability matrix  $P=p(H_{a}^{i}\,|\,H_{a}^{i-1},\hat{\theta})$.

We let $\Pi(a)$ denote the joint likelihood of a sequence of hidden states ($H_a$) and structural observations ($X_a$) conditional on a sequence of amino acids ($A_a$)  defined as follows:
\begin{align}
\label{eq:proteinlikelihood}
\Pi(a) = p(H_{a},A_{a},X_{a}\,|\,\hat{\theta}) = \notag \\
 p\big(A_{a}^{1},X_{a}^{1}\,|\,H_{a}^{1},\hat{\theta}\big)p(H_{a}^{1}\,|\,\hat{\theta})\notag \\
\times \prod_{i=2}^n p\big(A_{a}^{i},X_{a}^{i}\,|\,H_{a}^{i},\hat{\theta}\big)p(H_{a}^{i}\,|\,H_{a}^{i-1},\hat{\theta}),
\end{align}
where $p(H_{a}^{1}\,|\,\hat{\theta})$ is the initial probability of starting in state $H_{a}^{1}$ at the first site. Whilst  $\Pi(a)$ describes a single protein, it is used in the next section to `weight' an evolutionary model such that protein evolutionary trajectories are visited with probability proportional to $\Pi\times{\pi}$, where $\pi$ is weighting corresponding to the amino acid sequences.
\begin{comment}
The probability, $\Pi(a)$, of a set structural observations, $X_a$ conditioned on the corresponding hiddem state sequence, $H_a$,  the amino acid sequence, $S_a$, and model parameters, $\hat{\theta}$, is given by:
\begin{align}
\Pi(a) = p(X_{a}\,|\,H_{a},S_{a},\hat{\theta}) = \notag \\
\frac{1}{Z} p\big(X_{a}^{i}\,|\,H_{a}^{1},S_{a}^{1},\hat{\theta}\big)p(H_{a}^{1},\hat{\theta})\notag \\
\times \prod_{i=2}^n p\big(X_{a}^{i}\,|\,H_{a}^{i},S_{a}^{i},\hat{\theta}\big)p(H_{a}^{i}\,|\,H_{a}^{i-1},\hat{\theta}) 
\end{align}

where the normalising constant:
\begin{align}
Z = \,\sum_{H} \big[ p\big(X_{a}^{i}\,|\,H_{a}^{1},S_{a}^{1},\hat{\theta}\big)p(H_{a}^{1},\hat{\theta})\notag \\
\times \prod_{i=2}^n p\big(X_{a}^{i}\,|\,H_{a}^{i},S_{a}^{i},\hat{\theta}\big)p(H_{a}^{i}\,|\,H_{a}^{i-1},\hat{\theta}) \big]
\end{align}
where the sum runs over all possible hidden state sequences and can be calculated in $\mathcal{O}(h^2|P_{a}|)$ computational steps using the HMM forward algorithm.
\end{comment}

\subsubsection{Evolutionary model}
\begin{figure*}
	\centering
	\includegraphics[width=2.0\columnwidth]{figures/rate-network-withstructure.pdf}
	\caption{Will replace this figure with a better one, this is here to explain angle evolution}%
	\label{fig:ratenetwork}%
\end{figure*}
Thus far we have only considered a single protein. In this section we outline how multiple phylogenetically related proteins are modelled evolutionarily. Following \citet{choi2008basing} we construct a rate matrix that represents changes between two sequences, $a$ and $b$, instead of character states, such as amino acids, as is typical of substitution models. Furthermore, each sequence position combines a hidden state ($h_i$), an amino acid ($aa_i$), and a set of structural observations ($x_i$), into a joint character state ($h^{s}_i,aa^{s}_i,x^{s}_i$), where $s$ refers to a particular sequence. The rate matrix is given as follows:
\begin{equation}
\label{eq:ratematrix}
R_{ab}=\begin{cases}

\sqrt{\frac{\Pi(b)}{\Pi(a)}}U_{a_{i}b_{i}}\pi^{h^{b}_{i}}_{aa^{b}_{i}} & \text{Single amino acid}\\
& \text{difference at site $i$.}\\ 

\sqrt{\frac{\Pi(b)}{\Pi(a)}}V_{a_{i}b_{i}}\pi^{h^{b}_{i}}_{aa^{b}_{i}} & \text{Single hidden state}\\
& \text{difference at site $i$.}\\ 

0 & \text{Both hidden state and}\\
& \text{amino acid differences}\\
& \text{at site $i$.}\\ 

0 & \text{Differences at two}\\
& \text{or more sites.}\\

-\underset{k\neq a}{\sum}R_{ak} & a=b
\end{cases}
\end{equation}

where $i$ is the position that differs between sequence $a$ and $b$, $U$ is a symmetric $20\times{20}$ amino acid exchangeability matrix, and $V$ is a symmetric $h\times{h}$ hidden state exchangeability matrix. The term, $\frac{\Pi(b)}{\Pi(a)}$, weights each hidden state or amino acid change, such that sequences are visited with probability given by it's probability under the hidden Markov model multiplied by the amino acid sequence probability. This gives an evolutionary model on amino acid sequences and protein structures that accounts for neighbouring dependencies between adjacent sites and introduces temporal evolutionary dependencies between proteins.

The evolutionary dependencies between structures is introduced via the hidden states, thus avoiding having to directly implement an evolutionary process on structure, which is cumbersome given the continuous nature of the structural observations. We have previously developed a continuous diffusion process on angles for modelling protein dihedral angles, \citep{Garcia-Portugues:ads, Golden2017}, however,  protein backbone angles and bond lengths do not evolve in a continuous fashion, rather they are expected to 'jump' when changes occur. The hidden states capture this jump behaviour.
 
Note that proteins $P_a$ and $P_b$ referred to in the ratio $\frac{\Pi(b)}{\Pi(a)}$ in Equation~\ref{eq:ratematrix} always differ at exactly one site, implying that at most three terms in Equation~\ref{eq:proteinlikelihood} need to be considered when computing the ratio. 

Additionally note, although the summation in Equation~\ref{eq:ratematrix} appears to involve an exponential number of terms, most terms are equal to zero, except those that differ from $P_a$ at one position. Furthermore, an amino acid transition and a hidden state transition are not permitted to occur simulatenously, further reducing the number of terms that are summed ($19n$ amino acid terms plus $(h-1)n$ hidden state terms).


\paragraph{Stationary probability of proteins}
Following \citet{choi2008basing}, and by construction, the stationary probability of a protein $a$ is given by:
\begin{equation}
\label{eq:stationary_dist}
p(P_a|\hat{\theta}) = \frac{ \Pi(a) \underset{i}{\prod}\pi^{h^{a}_{i}}_{aa^{a}_{i}} }{ \sum_{k} \Pi(k) \underset{i}{\prod}\pi^{h^{k}_{i}}_{aa^{k}_{i}} } 
\end{equation}


\paragraph{Time-reversibility}
Since $U$ and $V$ in \eqref{eq:ratematrix} are symmetric matrices, i.e. $U_{a_{i}b_{i}}=U_{b_{i}a_{i}}$ and $V_{a_{i}b_{i}}=V_{b_{i}a_{i}}$, time-reversibility of the model holds, in other words:
\begin{equation}
p(P_a|\hat{\theta})M_{ab} = p(P_b|\hat{\theta})M_{ba}.
\end{equation}
Time-reversibility implies that at any rooting of the tree can be used if the equilibrium probabilities are taken to be the initial probabilities \citep{felsenstein1981evolutionary}, which is indeed the case for our model.

\paragraph{Dataset likelihood}
The likelihood of a given dataset $\mathcal{D}_d$ of proteins related by a tree $\mathcal{T}_d$ consisting of a set of branch paths $\mathcal{B}_d$  is given as follows:
\begin{align}
p(\mathcal{D}_d|\mathcal{T}_d,\mathcal{B}_d,\hat{\theta})=\notag\\
p(P_{root})\prod_{b \in \mathcal{B}_d} p\big(X_{b}(t_{end})\,|\,H_{b}(t_{end}),A_{b}(t_{end}),\hat{\theta}\big)\notag\\ 
\times \big[e^{R_{b_{n}}(t_{end}-t_{n})} \prod_{k=1}^{n}  e^{-R_{b_{k}b_{k}}(t_{k}-t_{k-1})} R_{b_{k-1}b_{k}} \big].
\end{align}
The first term, $p(P_{root})$, is the probability of the protein, $P_{root}$, at the root of the tree. The outermost product is a product over branches in $\mathcal{B}_d$, where the first term is the likelihood of any structural observations at the tip of each branch. The terms in square brackets represent the likelihood of the the hidden states and amino acids along a branch paths, as specified the rate matrix $R$. The first term in square parentheses is the probability that no events occur after the last event in a given branch path, whereas the second term is the probability of the events in a branch path and the waiting times between them.

\subsection{Model parameters}
The number of model parameters is a function of $h$, the number of hidden states:
\begin{equation*}
\begin{array}{ccc}
	\text{vM for }\omega: & 20\times2\times h\\
	\text{bvM for }\phi,\psi: & 20\times5\times h\\
	\text{vMs for }\tau_{1},\tau_{2},\tau_{3}: & 3\times2\times h\\
	\text{MVN for }b_{1},b_{2},b_{3} & 3\times8\times h\\
	\text{AA freqs.} & (20-1)\times h\\
	\text{AA exchangeability matrix} & 190\\
	\text{Hidden transition prob. matrix} & h^{2}-h\\
	\text{Hidden rate matrix} & (h^{2}-h)/2
\end{array}
\end{equation*}

\subsection{Inference}

\subsubsection{Branch path inference: a phylogeny}
Inference for a given dataset $\mathcal{D}_{d}$ consists of sampling the set of branch paths, $\mathcal{B}_{d}$, conditional on the tree topology and branch lengths, $\mathcal{T}_{d}$, and model parameters $\hat{\theta}$:
\begin{equation}
\mathcal{B}_{d} \sim p(\mathcal{B}_{d}|\mathcal{D}_{d},\mathcal{T}_d,\hat{\theta}).
\end{equation}
To sample this distribution, for each site $i$,  Felsenstein's algorithm was used to calculate likelihoods in a forward pass up the tree, followed by a backwards sampling pass down the tree to propose new hidden node states at the tip of each branch. The hidden state rate matrices used were conditional on the amino acid branch paths at site $i$ and hidden state branch paths at site $i-1$ and $i+1$.

Conditional upon proposed internal node states, modified rejection sampling was used to sample hidden state branch paths using the parent branch's proposed hidden node tip state as the start state and the current branches proposed hidden node tip state as the end state.

The proposed branch paths for site $i$ were then accepted or rejected using the Metropolis-Hastings ratio together with proposal ratio.

An analogous algorithm was used for sampling the amino acid branch paths, where the amino acid rate matrices used were conditional on the hidden states branch paths at each site $i$.

\subsubsection{Inference: a single protein}
Inference for a dataset consisting of a single protein $P_{a}$ is simpler and has a reduced computational cost, the reason for this is that the protein is assumed to be drawn from the stationary distribution of the model, which is given by \eqref{eq:stationary_dist}:
\begin{equation}
H_{a} \sim p(H_{a}|A_{a},\hat{\theta}).
\end{equation}
This distribution can be sampled exactly using the forward-filtering backward-sampling algorithm for HMMs \citep{fruhwirth1994data} in $\mathcal{O}(h^{2}|P_{a}|)$ computational time. Note that the amino acid sequence, $A_{a}$, is typically observed However, regardless of which combinations of $A_{a}$ and $X_{a}$ are observed it remains possible to use the forward-filtering backward-sampling algorithm to efficiently sample $H_{a}$.

\subsubsection{Backbone structure inference}
Branch path inference gives the distribution of $S_{b}(t)=\big(H_{b}(t),A_{b}\big)(t))$ at every point in time $t$ along a branch $b$. Conditioned on $H_{b}(t)$ and $A_{b}(t)$ the angles and bond lengths, $X_{b}(t)$, comprising the backbone structure can be trivially sampled using Equations~\ref{eq:omega_dist}-\ref{eq:bond_density}. The posterior marginal $p(X_{b}(t)|\mathcal{D}_{b},\mathcal{T}_{b})$ is therefore obtained by first sampling $H_{b}(t)$ and $A_{b}(t)$:
\begin{equation}
\big(H_{b}(t),A_{b}(t)\big) \sim p(H_{b}(t),A_{b}(t)|\mathcal{D}_{b},\mathcal{T}_{b}),
\end{equation}
followed by sampling $X_{b}(t)$ conditional on $H_{b}(t),A_{b}(t)$:
\begin{equation}
X_{b}(t) \sim p(X_{b}(t)|H_{b}(t),A_{b}(t)).
\end{equation}
% $p(H_{b}(t),A_{b}(t)|\mathcal{D}_{b},\mathcal{T}_{b})$, followed by sampling $p(X_{b}(t)|H_{b}(t),A_{b})$.

\subsection{Model training}

\subsubsection{Datasets}
\begin{table*}%	
	\centering
	\caption{\label{tab:datasets} Composition of training datasets}
	\begin{tabularx}{0.91\linewidth}{lccc}
		\toprule 
		Category & Number in & Amino acid & Structural\\
		& category & observations & observations\\
		\midrule
		\rowcolor{black!20} One sequence (no structure) &  310 & 84,806 & 0\\
		Two sequences (no structures) & 208 & 138,892 & 0\\
		\rowcolor{black!20} Three or more sequences (no structures) & 118 & 390,913 & 0\\
		One structure (one corresponding sequence) & 4,565 & 1,259,635 & 1,259,342\\
		\rowcolor{black!20} One structure (two or more sequences) &  192 & 147,679 & 58,887\\
		Two structures (two or more sequences) &  81 & 72,331 & 50,982\\
		\rowcolor{black!20} Three or more structures (three or more sequences) & 18 & 44,273 & 20,745\\
		\midrule
		Total & 5,492 & 2,138,529 & 1,389,956\\
		\bottomrule
	\end{tabularx}
\end{table*}

\subsubsection{Model estimation}
Stochastic EM (StEM, \citet{gilks1995markov}) was used to train the model. StEM  is a stochastic version of the well known Expectation-Maximization algorithm \citep{gilks1995markov}. Its distinguishing feature is that the E-step consists of filling in the values of the latent variables using sampling. Only a single value is sampled. StEM is attractive due to its computational efficiency and its tendency to avoid getting stuck in local minima \citep{gilks1995markov}.

Sampling was used in the E-step to sample branch paths and times. In other words, at iteration $k$ for each dataset, $d$, consisting of an aligned set of proteins, $\mathcal{D}_d$,  and a corresponding set of branch paths, $\mathcal{B}_d$, we draw samples, from the following joint-distribution:
\begin{align*}
Z_{d}^{(k)}\sim p(\mathcal{B}_d|\mathcal{D}_d,\Psi^{(k)}).
\end{align*}

In the M-step the samples from the previous E-step, were used to update the hidden node parameters ($\hat{\Psi}$) using efficient sufficient statistics (ESSs).


\iffalse
\begin{table*}
	\captionsetup{justification=centering}
	\caption{\label{tab:sequencereconstruction} Ancestral sequence reconstruction benchmarks}	
	\begin{tabularx}{1.0\linewidth}{ccccc}
	\toprule
	Dataset & Our model & LG2008 & BEAST & ASR\\
	\midrule
	\rowcolor{black!20} Influenza & 0.896 & 0.876 & 0.88 & 0.87\tabularnewline
	HIV & 0.896 & 0.876 & 0.88 & 0.87\tabularnewline
	\bottomrule
	\end{tabularx}
\end{table*}
\fi

\subsection{Calculation of angular distances}\label{sec:angulardistance}
For benchmarking and comparison purposes, the angular cosine distance was used to measure distances between pairs of dihedral angles, $\langle\phi_{a},\psi_{a}\rangle$ and $\langle\phi_{b},\psi_{b}\rangle$. It is defined as follows \citep{downs2002circular}:
\begin{align}
d(\langle\phi_{a},\psi_{a}\rangle,\langle\phi_{b},\psi_{b}\rangle)\notag\\
=\sqrt{4-2\cos(\phi_{a}-\phi_{b})-2\cos(\psi_{a}-\psi_{b})}.
\label{eq:angular_distance}
\end{align}
The maximum possible distance ($\sqrt{8} \approx 2.828$) occurs when $\phi_{a}-\phi_{b}=\pi$ and  $\psi_{a}-\psi_{b}=\pi$. Angular distance defined in this way has the property that when $\phi_{a}-\phi_{b}\approx 0$ and $\psi_{a}-\psi_{b}\approx 0$ are near zero it may be approximated by the Euclidean distance -- using the small angle approximation for cosine ($\cos\theta\approx1-\frac{\theta^{2}}{2}$  when $\theta$ is near zero):
\begin{align*}
d(\langle\phi_{a},\psi_{a}\rangle,\langle\phi_{b},\psi_{b}\rangle)\\ \approx\sqrt{4-2(1-(\phi_{a}-\phi_{b})^{2}/2)-2(1-(\psi_{a}-\psi_{b})^{2}/2)}\\
=\sqrt{(\phi_{a}-\phi_{b})^{2}+(\psi_{a}-\psi_{b})^{2}}.
\end{align*}

%%% Results and discussion %%%
\section{Results and Discussion}

\subsection{Benchmarks of ancestral sequence reconstruction}
\begin{figure*}
	\centering
	\includegraphics[width=2.0\columnwidth]{figures/benchmarks-target-plot.pdf}
	\caption{\revcom{Example target diagrams} The angular cosine distance \eqref{eq:angular_distance} was used to compare our model's predictions of phi-psi angles in PDB 5KON under four increasingly informative observation conditions (1-4). The plots show the percentage of phi-psi angles correctly predicted within five different cosine angular distance radii (0.25, 0.5, 1.0, 1.5, and 2.0; dashed blue lines). If the centre of the plot is taken to be location of each of the true phi-psi angles in PDB 5KON, the objective is to the maximise the percentage of angles predicted within the innermost radius.}%
	\label{fig:benchmarkstarget}%
\end{figure*}

\subsubsection{Benchmarks of homology modelling}
\begin{table*}%	
	\centering
	\caption{\label{tab:benchmarkshomology} Benchmarks}
	\begin{tabularx}{1.0\linewidth}{lccccccc}
		\toprule 
		 & & & Branch & & & & Mean \\
		 & Missing  & Conditioning &  distance to &  & & & angular\\
		Dataset & structure & information &  homologue & $r < 0.25$ & $r < 0.5$  & $r < 1.0$ & distance\\
		\midrule
 		& 4OOV & 4RPD & 0.70 & 24.2\% & 52.7\% & 77.6\% & 0.71\\
 		& 5KON & 4RPD & 0.75 & 23.6\% & 52.1\% & 77.0\% & 0.72\\
 		& 3SKB & 4RPD & 0.71 & 20.4\% & 49.0\% & 76.7\% & 0.73\\
 		& 4RPD & 4RPD & 0.00 & 35.0\% & 71.7\% & 94.7\% & 0.41\\
 		& 4OOS & 4RPD & 0.76 & 23.8\% & 51.9\% & 77.0\% & 0.72\\
		& 4OOV & 4OOS & 0.12 & 32.2\% & 67.1\% & 90.6\% & 0.49\\
 		& 5KON & 4OOS & 0.18 & 33.1\% & 69.0\% & 92.9\% & 0.45\\
 		& 3SKB & 4OOS & 0.13 & 26.7\% & 61.7\% & 90.3\% & 0.52\\
 		& 4RPD & 4OOS & 0.76 & 23.4\% & 52.6\% & 78.1\% & 0.71\\
 		& 4OOS & 4OOS & 0.00 & 35.6\% & 72.5\% & 95.3\% & 0.40\\
		\bottomrule
	\end{tabularx}
\end{table*}
	
%%% Conclusions %%%
\section{Conclusions}

%%% Software availability %%%
\section{Software availability}
Julia code (compatible with Windows and Linux) is available at: \href{https://github.com/michaelgoldendev/protein-evolution}{https://github.com/michaelgoldendev/protein-evolution}

%%% Acknowledgements %%%
\section{Acknowledgements}
MG is supported by the ERC under the European Union’s Seventh Framework Programme (FP7/2007-2013)/ERC grant agreement no. 614725-PATHPHYLODYN. 

\ifmbeformat
\section{Supplementary material}
Supplementary material is available  at Molecular Biology and Evolution
online: \url{http://www.mbe.oxfordjournals.org/}
\fi

\bibliographystyle{natbib}%%%%natbib.sty
\bibliography{refs}%%%refs.bib


\end{document}